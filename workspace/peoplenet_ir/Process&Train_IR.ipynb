{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4238124e",
   "metadata": {},
   "source": [
    "## Process and Run Experiment on the Infrared Dataset\n",
    "\n",
    "This notebook contains all the code needed to train PeopleNet using TAO on an infrared dataset.\n",
    "\n",
    "This notebook requires the TAO Launcher, Docker and NGC to be setup\n",
    "\n",
    "The github readme has links to setup the prerequisites\n",
    "\n",
    "This notebook requires the FLIR dataset split archives to be downloaded and placed in ../../datasets/infrared\n",
    "\n",
    "It can be found here https://www.flir.com/oem/adas/adas-dataset-form/\n",
    "\n",
    "\n",
    "This notebook takes the following steps\n",
    "\n",
    "1) Combine and unzip the FLIR dataset  \n",
    "2) Convert the dataset labels into kitti labels  \n",
    "3) Move images and labels into a kitti directory structure  \n",
    "4) Map local directories the the TAO launcher\n",
    "5) Use TAO Offline Augmentation to resize the images  \n",
    "6) Split the dataset into a test set and 20%, 40%, 60% and 80% train subsets  \n",
    "7) Generate TF records for test set and all training sets  \n",
    "8) Download pretrained PeopleNet weights  \n",
    "9) Train models with and without PeopleNet weights  \n",
    "10) Graph Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#paths relative to local repository\n",
    "repo_home = os.path.join(os.getcwd(), \"../../\")\n",
    "model_home = os.path.join(repo_home, \"workspace/models\")\n",
    "dataset_home = os.path.join(repo_home, \"datasets/infrared\")\n",
    "exp_home = os.path.join(repo_home, \"workspace/peoplenet_ir\")\n",
    "\n",
    "#paths for inside the container\n",
    "dataset_home_cont = \"/datasets/infrared/\"\n",
    "exp_home_cont = \"/tlt_exp/peoplenet_ir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79b024",
   "metadata": {},
   "source": [
    "### Combine and unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $dataset_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the zip file together and unpack\n",
    "!cat FLIR_ADAS_1_3.zip* > FLIR_combined.zip\n",
    "!unzip FLIR_combined.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d6630",
   "metadata": {},
   "source": [
    "### Convert to kitti labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bc36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the json data that contains truth data\n",
    "import json\n",
    "\n",
    "js_file_path = os.path.join(dataset_home, \"FLIR_ADAS_1_3/train/thermal_annotations.json\")\n",
    "js_file = open(js_file_path, \"r\")\n",
    "label_data = json.load(js_file)\n",
    "js_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the json data into kitti labels\n",
    "import os\n",
    "ir_kitti = os.path.join(dataset_home, \"ir_kitti\")\n",
    "label_out = os.path.join(ir_kitti, \"labels\")\n",
    "os.makedirs(label_out, exist_ok=True)\n",
    "for label in label_data[\"annotations\"]:\n",
    "    label_name = \"FLIR_\" + str(label[\"image_id\"] + 1).zfill(5) + \".txt\"\n",
    "    cat_id = label[\"category_id\"]\n",
    "    with open(os.path.join(label_out, label_name), \"a+\") as file:\n",
    "        cat = label_data[\"categories\"][cat_id-1][\"name\"]\n",
    "        xmin = label[\"bbox\"][0]\n",
    "        ymin = label[\"bbox\"][1]\n",
    "        xmax = xmin + label[\"bbox\"][2]\n",
    "        ymax = ymin + label[\"bbox\"][3]\n",
    "        file.write(f\"{cat} 0 0 0 {xmin} {ymin} {xmax} {ymax} 0 0 0 0 0 0 0\\n\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfff9d9",
   "metadata": {},
   "source": [
    "### Move images and labels into a kitti directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy images that match with a label into the kitti image folder\n",
    "import shutil\n",
    "image_out = os.path.join(ir_kitti, \"images\")\n",
    "os.makedirs(image_out, exist_ok=True)\n",
    "\n",
    "for label in os.listdir(label_out):\n",
    "    image_name1 = label.split(\".\")[0] + \".jpeg\"\n",
    "    image_name2 = label.split(\".\")[0] + \".jpg\"\n",
    "    shutil.copy(os.path.join(dataset_home,\"FLIR_ADAS_1_3/train/thermal_8_bit\", image_name1), os.path.join(image_out, image_name2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276df0c3",
   "metadata": {},
   "source": [
    "## Map local directories the the TAO launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f75daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.path.join(repo_home, \"datasets\"),\n",
    "            \"destination\": \"/datasets\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.path.join(repo_home, \"workspace\"),\n",
    "            \"destination\": \"/tlt_exp\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d112b",
   "metadata": {},
   "source": [
    "### Use TAO Offline Augmentation to resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the augmentation spec file\n",
    "img_x = 960\n",
    "img_y = 544\n",
    "ext = \".jpg\"\n",
    "aug_spec = f\"\"\"\n",
    "    # Setting up dataset config.\n",
    "    dataset_config{{\n",
    "      image_path: \"images\"\n",
    "      label_path: \"labels\"\n",
    "    }}\n",
    "    output_image_width: {img_x}\n",
    "    output_image_height: {img_y}\n",
    "    output_image_channel: 3\n",
    "    image_extension: \"{ext}\"\n",
    " \"\"\"\n",
    "aug_spec_path = os.path.join(ir_kitti, \"aug_spec.txt\")\n",
    "with open(aug_spec_path, \"w+\") as file:\n",
    "    file.write(aug_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8786fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run augment command\n",
    "ir_kitti_cont = os.path.join(dataset_home_cont, \"ir_kitti\")\n",
    "aug_spec_path_cont = os.path.join(ir_kitti_cont, \"aug_spec.txt\")\n",
    "resized_output_cont = os.path.join(dataset_home_cont,\"ir_resized\")\n",
    "resized_output = os.path.join(dataset_home,\"ir_resized\")\n",
    "\n",
    "!tao augment -d $ir_kitti_cont -a $aug_spec_path_cont -o $resized_output_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd6c51",
   "metadata": {},
   "source": [
    "### Split the dataset into a test set and 20%, 40%, 60% and 80% train subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b168e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(original, name_list, output_folder):\n",
    "    \n",
    "    #determine image ext\n",
    "    ext = os.path.splitext(os.listdir(os.path.join(original, \"images\"))[0])[1]\n",
    "    \n",
    "    \n",
    "    image_out = os.path.join(output_folder, \"images\")\n",
    "    label_out = os.path.join(output_folder, \"labels\")\n",
    "    \n",
    "    os.makedirs(image_out, exist_ok=True)\n",
    "    os.makedirs(label_out, exist_ok=True)\n",
    "    \n",
    "    with open(name_list, \"r\") as ls:\n",
    "        for line in ls:\n",
    "            line = line.strip()\n",
    "            shutil.copy(os.path.join(original,\"images\", line + ext), os.path.join(image_out, line + ext))\n",
    "            shutil.copy(os.path.join(original, \"labels\", line + \".txt\"), os.path.join(label_out, line + \".txt\"))\t\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_lists = [\"test_set.txt\", \"train_20.txt\", \"train_40.txt\", \"train_60.txt\", \"train_80.txt\"]\n",
    "for list_file in subset_lists:\n",
    "    output_name = list_file.split(\".\")[0]\n",
    "    output_path = os.path.join(dataset_home, output_name)\n",
    "    input_list_path = os.path.join(exp_home, list_file)\n",
    "    create_subset(resized_output, input_list_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013fdf63",
   "metadata": {},
   "source": [
    "### Generate TF records for test set and all training sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78998b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def gen_tf_spec(dataset_path):\n",
    "\n",
    "    spec_str = f\"\"\"\n",
    "    kitti_config {{\n",
    "      root_directory_path: \"{dataset_path}\"\n",
    "      image_dir_name: \"images\"\n",
    "      label_dir_name: \"labels\"\n",
    "      image_extension: \".jpg\"\n",
    "      partition_mode: \"random\"\n",
    "      num_partitions: 2\n",
    "      val_split: 20\n",
    "      num_shards: 10\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return spec_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0da133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the spec and generate tf records for all sets\n",
    "datasets = [\"test_set\", \"train_20\", \"train_40\", \"train_60\", \"train_80\"]\n",
    "for path in datasets:\n",
    "    dataset_path = os.path.join(dataset_home, path)\n",
    "    dataset_path_cont = os.path.join(dataset_home_cont, path)\n",
    "    \n",
    "    record_path = os.path.join(dataset_path, \"tfrecord_spec.txt\")\n",
    "    record_path_cont = os.path.join(dataset_path_cont, \"tfrecord_spec.txt\")\n",
    "    \n",
    "    record_output = os.path.join(dataset_path_cont, \"tfrecords/\")\n",
    "    \n",
    "    with open(record_path, \"w+\") as spec:\n",
    "        spec.write(gen_tf_spec(dataset_path_cont))\n",
    "    !tao detectnet_v2 dataset_convert -d $record_path_cont -o $record_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292b412",
   "metadata": {},
   "source": [
    "### Download pretrained PeopleNet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cd972",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $model_home\n",
    "!ngc registry model download-version \"nvidia/tlt_peoplenet:unpruned_v2.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066860d1",
   "metadata": {},
   "source": [
    "### Train models with and without PeopleNet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46819984",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e /tlt_exp/peoplenet_ir/experiments/peoplenet_20/training_spec.txt -r /tlt_exp/peoplenet_ir/experiments/peoplenet_20 -n \"final_model\" -k \"tlt_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e /tlt_exp/peoplenet_ir/experiments/peoplenet_40/training_spec.txt -r /tlt_exp/peoplenet_ir/experiments/peoplenet_40 -n \"final_model\" -k \"tlt_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e /tlt_exp/peoplenet_ir/experiments/peoplenet_60/training_spec.txt -r /tlt_exp/peoplenet_ir/experiments/peoplenet_60 -n \"final_model\" -k \"tlt_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7058100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e /tlt_exp/peoplenet_ir/experiments/peoplenet_80/training_spec.txt -r /tlt_exp/peoplenet_ir/experiments/peoplenet_80 -n \"final_model\" -k \"tlt_encode\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6e9fc",
   "metadata": {},
   "source": [
    "Train without peoplenet (random starting weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f23b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e /tlt_exp/peoplenet_ir/experiments/random_20/training_spec.txt -r /tlt_exp/peoplenet_ir/experiments/random_20 -n \"final_model\" -k \"tlt_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493cc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e /tlt_exp/peoplenet_ir/experiments/random_40/training_spec.txt -r /tlt_exp/peoplenet_ir/experiments/random_40 -n \"final_model\" -k \"tlt_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e /tlt_exp/peoplenet_ir/experiments/random_60/training_spec.txt -r /tlt_exp/peoplenet_ir/experiments/random_60 -n \"final_model\" -k \"tlt_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e /tlt_exp/peoplenet_ir/experiments/random_80/training_spec.txt -r /tlt_exp/peoplenet_ir/experiments/random_80 -n \"final_model\" -k \"tlt_encode\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f21c7",
   "metadata": {},
   "source": [
    "### Graph Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e45eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_data(filepath):\n",
    "    x_vals_map = []\n",
    "    y_vals_map = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        epoch = 0\n",
    "        for line in f:\n",
    "            data = eval(line)\n",
    "            if \"cur_epoch\" in data.keys():\n",
    "                epoch = data[\"cur_epoch\"]\n",
    "\n",
    "            elif \"mean average precision\" in data.keys():\n",
    "                mAP = data[\"mean average precision\"]\n",
    "                y_vals_map.append(mAP)\n",
    "                x_vals_map.append(epoch)\n",
    "\n",
    "    return x_vals_map, y_vals_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd07c8",
   "metadata": {},
   "source": [
    "PeopleNet mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607130d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = os.path.join(exp_home, \"peoplenet_20/status.json\")\n",
    "f2 = os.path.join(exp_home, \"peoplenet_40/status.json\")\n",
    "f3 = os.path.join(exp_home, \"peoplenet_60/status.json\")\n",
    "f4 = os.path.join(exp_home, \"peoplenet_80/status.json\")\n",
    "\n",
    "files = [f4,f3,f2,f1] #modify for trainings that are complete\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('PeopleNet on IR Data \\n mAP Over Epoch')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"mAP %\")\n",
    "plt.ylim([0,100])\n",
    "plt.yticks(range(0,101,10))\n",
    "plt.tick_params(right=True, labelright=True)\n",
    "\n",
    "for f in files:\n",
    "    x,y = get_map_data(f)\n",
    "    print(f + \"\\n max mAP: \" + str(max(y)) + \" \\n\")\n",
    "    plt.plot(x,y)\n",
    "\n",
    "leg = [\"x4\", \"x3\", \"x2\", \"x1\"]\n",
    "plt.legend(leg, title=\"Dataset Size \\nx1 = 1,572\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59babbf5",
   "metadata": {},
   "source": [
    "Without PeopleNet mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad843c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = os.path.join(exp_home, \"random_20/status.json\")\n",
    "f2 = os.path.join(exp_home, \"random_40/status.json\")\n",
    "f3 = os.path.join(exp_home, \"random_60/status.json\")\n",
    "f4 = os.path.join(exp_home, \"random_80/status.json\")\n",
    "\n",
    "files = [f4,f3,f2,f1] #modify for trainings that are complete\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Without PeopleNet on IR Data \\n mAP Over Epoch')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"mAP %\")\n",
    "plt.ylim([0,100])\n",
    "plt.yticks(range(0,101,10))\n",
    "plt.tick_params(right=True, labelright=True)\n",
    "\n",
    "for f in files:\n",
    "    x,y = get_map_data(f)\n",
    "    print(f + \"\\n max mAP: \" + str(max(y)) + \" \\n\")\n",
    "    plt.plot(x,y)\n",
    "\n",
    "leg = [\"x4\", \"x3\", \"x2\", \"x1\"]\n",
    "plt.legend(leg, title=\"Dataset Size \\nx1 = 1,572\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
