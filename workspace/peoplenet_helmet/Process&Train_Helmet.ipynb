{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581ab908",
   "metadata": {},
   "source": [
    "## Process and Run Experiment on the Helmet Dataset\n",
    "\n",
    "This notebook contains all code needed to train use Peoplenet to inference a dataset and then train with an added class\n",
    "\n",
    "This notebook requires the TAO Launcher, Docker and NGC to be setup\n",
    "\n",
    "The notebook also requires a helmet dataset from kaggle which can be found here https://www.kaggle.com/andrewmvd/helmet-detection\n",
    "\n",
    "It must be downloaded and the archive.zip placed in ../../datasets/helmet\n",
    "\n",
    "This notebook takes the following steps\n",
    "\n",
    "1) Clear past results  \n",
    "2) Convert the dataset into kitti format  \n",
    "3) Download PeopleNet model  \n",
    "4) Map local directories the the TAO launcher  \n",
    "5) Inference the images with PeopleNet and combine labels  \n",
    "6) Generate TF Records  \n",
    "7) Train PeopleNet with the helmet class  \n",
    "8) Graph Results  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ee447",
   "metadata": {},
   "source": [
    "### Clear past results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ceb82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to clear past results\n",
    "#May need to run the following command outside the noteobok: sudo chown -R $USER /path/to/github/repo\n",
    "!rm -rf $dataset_home/helmet_set_all \n",
    "!rm -rf $dataset_home/annotations \n",
    "!rm -rf $dataset_home/images \n",
    "!rm -rf $dataset_home/peoplenet_labels \n",
    "!rm -rf $exp_home/trained_inf_out \n",
    "!rm -rf $exp_home/train_out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9545355f",
   "metadata": {},
   "source": [
    "### Convert the dataset into kitti format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from preprocess_helmet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39870ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#local file paths\n",
    "repo_home = os.path.join(os.getcwd(), \"../../\")\n",
    "dataset_home = os.path.join(repo_home, \"datasets/helmet\")\n",
    "exp_home = os.path.join(os.getcwd(), \"experiments\")\n",
    "workspace_home = os.path.join(repo_home, \"workspace\")\n",
    "models_home = os.path.join(repo_home, \"workspace/models\")\n",
    "\n",
    "#file paths inside the container\n",
    "dataset_home_cont = \"/datasets/helmet\"\n",
    "exp_home_cont = \"/tlt_exp/peoplenet_helmet/experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $dataset_home\n",
    "!unzip $dataset_home/archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup output paths for labels and images\n",
    "label_out = os.path.join(dataset_home, \"helmet_set_all/labels\")\n",
    "image_out = os.path.join(dataset_home, \"helmet_set_all/images\")\n",
    "\n",
    "os.makedirs(image_out, exist_ok=True)\n",
    "os.makedirs(label_out, exist_ok=True)\n",
    "\n",
    "#move all the images to the right folder\n",
    "!mv $dataset_home/images/* $image_out\n",
    "\n",
    "#Convert xml labels to kitti txt file labels and put them in the output path\n",
    "xml_labels = os.path.join(dataset_home, \"annotations\")\n",
    "for label in os.listdir(xml_labels):\n",
    "    convert_annotation(os.path.join(xml_labels,label), label_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b090b3c",
   "metadata": {},
   "source": [
    "### Download PeopleNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(models_home, \"detectnet_v2\"), exist_ok=True)\n",
    "%cd $models_home/detectnet_v2\n",
    "!ngc registry model download-version \"nvidia/tlt_peoplenet:unpruned_v2.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f769fa0",
   "metadata": {},
   "source": [
    "## Map local directories the the TAO launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f977e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.path.join(repo_home, \"datasets\"),\n",
    "            \"destination\": \"/datasets\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.path.join(repo_home, \"workspace\"),\n",
    "            \"destination\": \"/tlt_exp\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4fe9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tlt_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292cd4f",
   "metadata": {},
   "source": [
    "### Inference the images with PeopleNet and combine labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ec4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to jpg\n",
    "#from PIL import Image\n",
    "#image_folder = os.path.join(dataset_home, \"helmet_set_all/images\")\n",
    "#for im_path in os.listdir(image_folder):\n",
    "#    im = Image.open(os.path.join(image_folder, im_path))\n",
    "#    im = im.convert(\"RGB\")\n",
    "#    im.save(os.path.join(image_folder, im_path.split(\".\")[0] + \".jpg\"))\n",
    "#\n",
    "#!rm $image_folder/*.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fffcac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup variables to inference the images with peoplenet\n",
    "inference_spec = os.path.join(exp_home_cont,\"inf_people_spec.txt\")\n",
    "output_folder = os.path.join(dataset_home_cont, \"peoplenet_labels\")\n",
    "dataset_images = os.path.join(dataset_home_cont, \"helmet_set_all/images\")\n",
    "key = \"tlt_encode\"\n",
    "\n",
    "#inference the helmet dataset images with peoplenet\n",
    "!tao detectnet_v2 inference -e $inference_spec -o $output_folder -i $dataset_images -k $key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the inferenced labels with the existing helmet labels\n",
    "output_folder = os.path.join(dataset_home, \"peoplenet_labels\")\n",
    "\n",
    "#loop through all labels in helmet dataset\n",
    "for label in os.listdir(os.path.join(dataset_home,\"helmet_set_all/labels\")):\n",
    "    helmet_label = os.path.join(dataset_home, \"helmet_set_all/labels\", label)\n",
    "    inferenced_labels = os.path.join(output_folder, \"labels\")\n",
    "    people_label = os.path.join(output_folder,\"labels\",label)\n",
    "    #append the peoplenet inference to the original helmet labels\n",
    "    with open(helmet_label, \"a\") as label_f:\n",
    "        with open(people_label, \"r\") as people_f:\n",
    "            for line in people_f:\n",
    "                line = line.split(\" \")\n",
    "                line = \" \".join(line[:-1])\n",
    "                label_f.write(line + \"\\n\")\n",
    "    #output is a label text file with the helmet, people and face labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68ef01",
   "metadata": {},
   "source": [
    "### Generate TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22280142",
   "metadata": {},
   "outputs": [],
   "source": [
    " def gen_tf_spec(dataset_path):\n",
    "\n",
    "    spec_str = f\"\"\"\n",
    "    kitti_config {{\n",
    "      root_directory_path: \"{dataset_path}\"\n",
    "      image_dir_name: \"images\"\n",
    "      label_dir_name: \"labels\"\n",
    "      image_extension: \".png\"\n",
    "      partition_mode: \"random\"\n",
    "      num_partitions: 2\n",
    "      val_split: 20\n",
    "      num_shards: 10\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return spec_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f982885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup paths for tf record generation\n",
    "path = os.path.join(dataset_home, \"helmet_set_all\")\n",
    "path_cont = os.path.join(dataset_home_cont, \"helmet_set_all\") #path inside container\n",
    "\n",
    "record_path = os.path.join(path, \"tfrecord_spec.txt\")\n",
    "\n",
    "#Write the tf record spec file\n",
    "with open(record_path, \"w+\") as spec:\n",
    "    spec.write(gen_tf_spec(path_cont))\n",
    "    \n",
    "record_output = os.path.join(path_cont, \"tfrecords/\") \n",
    "record_path = os.path.join(path_cont, \"tfrecord_spec.txt\")\n",
    "\n",
    "#Generate the tf records\n",
    "!tao detectnet_v2 dataset_convert -d $record_path -o $record_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbdba7",
   "metadata": {},
   "source": [
    "### Train PeopleNet with the helmet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ca9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup paths for training and inference\n",
    "train_out = os.path.join(exp_home, \"train_out\")\n",
    "train_out_cont = os.path.join(exp_home_cont, \"train_out\")\n",
    "train_spec_path = os.path.join(exp_home_cont, \"train_spec.txt\")\n",
    "inf_spec_path = os.path.join(exp_home_cont, \"inf_new_spec.txt\")\n",
    "model_out = os.path.join(exp_home_cont, \"train_out\")\n",
    "trained_model = os.path.join(model_out, \"final_model.tlt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43715378",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e $train_spec_path -r $train_out_cont -n \"final_model\" -k \"tlt_encode\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d43f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 inference -e $inf_spec_path -i \"/datasets/helmet/helmet_set_all/images\" -o \"/tlt_exp/peoplenet_helmet/experiments/trained_inf_out\" -k \"tlt_encode\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253ed7b",
   "metadata": {},
   "source": [
    "### Graph Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df122349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt   \n",
    "def get_map_data(filepath):\n",
    "    x_vals_map = []\n",
    "    y_vals_map = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        epoch = 0\n",
    "        for line in f:\n",
    "            data = eval(line)\n",
    "            if \"cur_epoch\" in data.keys():\n",
    "                epoch = data[\"cur_epoch\"]\n",
    "\n",
    "            elif \"mean average precision\" in data.keys():\n",
    "                mAP = data[\"average_precision\"][\"withhelmet\"]\n",
    "                y_vals_map.append(mAP)\n",
    "                x_vals_map.append(epoch)\n",
    "\n",
    "    return x_vals_map, y_vals_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_file = os.path.join(train_out, \"status.json\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Helmet AP Over Epoch')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AP %\")\n",
    "plt.ylim([0,100])\n",
    "plt.yticks(range(0,101,10))\n",
    "plt.tick_params(right=True, labelright=True)\n",
    "\n",
    "x,y = get_map_data(status_file)\n",
    "print(status_file + \"\\n max AP: \" + str(max(y)) + \" \\n\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc8b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
