{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e1cef3",
   "metadata": {},
   "source": [
    "# Data Augmenation On The PCB Defect Dataset\n",
    "\n",
    "This notebook contains all the code needed to use TLT augmenation on subsets of the PCB defect dataset to showcase how augmenatation can be used to improve KPIs for small datasets. \n",
    "\n",
    "This notebook is required to run in the TLT Stream Analysytics container which can be found here. https://ngc.nvidia.com/catalog/containers/nvidia:tlt-streamanalytics\n",
    "\n",
    "The github readme has steps on how pull and launch the container. \n",
    "\n",
    "This notebook also requires preprocess_pcb.py to be in the same directory to function. \n",
    "\n",
    "This notebook takes teh following steps\n",
    "1) Download and unpack the PCB defect dataset\n",
    "\n",
    "2) Convert the dataset to kitti format \n",
    "\n",
    "3) Split the dataset into test and train subsets\n",
    "\n",
    "4) Generate offline augmenation spec file and apply augmentation to the training sets\n",
    "\n",
    "5) Generate TF Records for the test and training sets\n",
    "\n",
    "6) Downloads pretrained object detection weights needed for the trainings\n",
    "\n",
    "7) Launch trainings and evaluation\n",
    "\n",
    "The last section of this notebook contains all the commands needed to run training and evaluation on all 6 datasets.  \n",
    "Steps 1-6 only need to run 1 time. The trainings in step 7 can be run in any order once steps 1-6 have successfully run. \n",
    "A common test set of 500 images is used for validation on all trainings\n",
    "\n",
    "100 subset x1  \n",
    "100 subset x10  \n",
    "100 subset x20  \n",
    "500 subset x1  \n",
    "500 subset x10  \n",
    "500 subset x20  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514856e",
   "metadata": {},
   "source": [
    "## Download and unpack the PCB defect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f31942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install matplotlib==3.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /datasets/pcb_defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/h0f39nyotddibsb/VOC_PCB.zip \n",
    "!unzip VOC_PCB.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd6e64",
   "metadata": {},
   "source": [
    "## Convert the dataset to kitti format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from preprocess_pcb import convert_annotation, create_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"original/images\", exist_ok=True)\n",
    "os.makedirs(\"original/labels\", exist_ok=True)\n",
    "!cp -r VOC_PCB/JPEGImages/. original/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2132174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Paths and make label folder\n",
    "xml_label_path = \"VOC_PCB/Annotations\"\n",
    "kitti_label_output = \"original/labels\"\n",
    "\n",
    "#Convert labels to kitti and put into output folder\n",
    "for x in os.listdir(xml_label_path):\n",
    "    current_label_path = os.path.join(xml_label_path, x)\n",
    "    convert_annotation(current_label_path, kitti_label_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99a767",
   "metadata": {},
   "source": [
    "## Split the dataset into test and train subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb578f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_500 = \"/tlt_exp/pcb_data_aug/test_500_list.txt\"\n",
    "train_100 = \"/tlt_exp/pcb_data_aug/train_100_list.txt\"\n",
    "train_500 = \"/tlt_exp/pcb_data_aug/train_500_list.txt\"\n",
    "\n",
    "\n",
    "os.makedirs(\"500_subset_test_x1\", exist_ok=True)\n",
    "os.makedirs(\"100_subset_train_x1\", exist_ok=True)\n",
    "os.makedirs(\"500_subset_train_x1\", exist_ok=True)\n",
    "\n",
    "create_subset(\"original\", test_500, \"500_subset_test_x1\")\n",
    "create_subset(\"original\", train_100, \"100_subset_train_x1\")\n",
    "create_subset(\"original\", train_500, \"500_subset_train_x1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd4fc5",
   "metadata": {},
   "source": [
    "## Generate offline augmenation spec file and apply augmentation to the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_pcb import gen_random_aug_spec, combine_kitti, visualize_images\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ea3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augments(dataset_folder, output_folder, num_augments):\n",
    "    for i in range(0,num_augments):\n",
    "        spec_out = os.path.join(output_folder, \"aug_spec\" + str(i) + \".txt\")\n",
    "        gen_random_aug_spec(600,600,\"jpg\", spec_out)\n",
    "        !cat $spec_out\n",
    "\n",
    "        aug_folder = os.path.join(output_folder, \"aug\" + str(i))\n",
    "        !augment -a $spec_out -o $aug_folder -d $dataset_folder\n",
    "\n",
    "        if i == 0:\n",
    "            d1 = dataset_folder\n",
    "            d2 = aug_folder\n",
    "            d3 = os.path.join(output_folder, \"combined_x2\")\n",
    "            combine_kitti(d1,d2,d3)\n",
    "        else:\n",
    "            d1 = os.path.join(output_folder, \"combined_x\" + str(i+1))\n",
    "            d2 = aug_folder\n",
    "            d3 = os.path.join(output_folder, \"combined_x\" + str(i+2))\n",
    "            combine_kitti(d1,d2,d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"100_subset_train_x1\" #folder for the existing dataset to be augmented. This folder will not be modified\n",
    "output_folder = \"100_subset_train_aug\" #folder for the augmented output. Does not need to exist\n",
    "num_augments = 19 #number of augmented datasets to generate\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "generate_augments(dataset_folder, output_folder, num_augments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7149675",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_choice = str(randint(0,num_augments-1))\n",
    "visualize_images(os.path.join(output_folder, \"aug\"+aug_choice+\"/images\"), num_images=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e720bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"500_subset_train_x1\" #folder for the existing dataset to be augmented. This folder will not be modified\n",
    "output_folder = \"500_subset_train_aug\" #folder for the augmented output. Does not need to exist\n",
    "num_augments = 19 #number of augmented datasets to generate\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "generate_augments(dataset_folder, output_folder, num_augments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9402e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_choice = str(randint(0,num_augments-1))\n",
    "visualize_images(os.path.join(output_folder, \"aug\"+aug_choice+\"/images\"), num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4a9ba",
   "metadata": {},
   "source": [
    "Place important datasets in the dataset folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv 100_subset_train_aug/combined_x10 /datasets/100_subset_train_x10\n",
    "!mv 100_subset_train_aug/combined_x20 /datasets/100_subset_train_x20\n",
    "\n",
    "!mv 500_subset_train_aug/combined_x10 /datasets/500_subset_train_x10\n",
    "!mv 500_subset_train_aug/combined_x20 /datasets/500_subset_train_x20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e8aaa",
   "metadata": {},
   "source": [
    "## Generate TF Records for the test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0905f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tf_spec(dataset_path):\n",
    "\n",
    "    spec_str = f\"\"\"\n",
    "    kitti_config {{\n",
    "      root_directory_path: \"/datasets/pcb_data_aug/{dataset_path}\"\n",
    "      image_dir_name: \"images\"\n",
    "      label_dir_name: \"labels\"\n",
    "      image_extension: \".jpg\"\n",
    "      partition_mode: \"random\"\n",
    "      num_partitions: 2\n",
    "      val_split: 20\n",
    "      num_shards: 10\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return spec_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [\"500_subset_test_x1\", \"500_subset_train_x1\", \"500_subset_train_x10\", \"500_subset_train_x20\", \"100_subset_train_x1\", \"100_subset_train_x10\", \"100_subset_train_x20\"]\n",
    "for path in dataset_paths:\n",
    "    record_path = os.path.join(\"/datasets/pcb_data_aug\", path, \"tfrecord_spec.txt\")\n",
    "    record_output = os.path.join(\"/datasets/pcb_data_aug\", path, \"tfrecords_rcnn/\")\n",
    "    print(\"************\" + record_path)\n",
    "    with open(record_path, \"w+\") as spec:\n",
    "        spec.write(gen_tf_spec(path))\n",
    "    !detectnet_v2 dataset_convert -d $record_path -o $record_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa807a1f",
   "metadata": {},
   "source": [
    "## Downloads pretrained object detection weights needed for the trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84508546",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/tlt_exp/models/fasterRCNN\", exist_ok=True)\n",
    "%cd /tlt_exp/models/fasterRCNN\n",
    "!ngc registry model download-version \"nvidia/tlt_pretrained_object_detection:resnet18\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7487f9",
   "metadata": {},
   "source": [
    "## Launch trainings and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030573a",
   "metadata": {},
   "source": [
    "Each cell in this section will train and evaluate on 1 dataset in the experiment. The results will be output to the respective experiment folder. \n",
    "\n",
    "The trainings may take several hours depending on your hardware. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /tlt_exp/pcb_data_aug/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778056e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_online_aug/100_subset_train_x1/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_online_aug/100_subset_train_x1/training_spec.txt -k tlt_encode --log_file offline_online_aug/100_subset_train_x1/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_online_aug/100_subset_train_x10/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_online_aug/100_subset_train_x10/training_spec.txt -k tlt_encode --log_file offline_online_aug/100_subset_train_x10/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_online_aug/100_subset_train_x20/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_online_aug/100_subset_train_x20/training_spec.txt -k tlt_encode --log_file offline_online_aug/100_subset_train_x20/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2257d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_online_aug/500_subset_train_x1/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_online_aug/500_subset_train_x1/training_spec.txt -k tlt_encode --log_file offline_online_aug/500_subset_train_x1/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef25d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_online_aug/500_subset_train_x10/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_online_aug/500_subset_train_x10/training_spec.txt -k tlt_encode --log_file offline_online_aug/500_subset_train_x10/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_online_aug/500_subset_train_x20/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_online_aug/500_subset_train_x20/training_spec.txt -k tlt_encode --log_file offline_online_aug/500_subset_train_x20/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d755c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_aug/100_subset_train_x1/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_aug/100_subset_train_x1/training_spec.txt -k tlt_encode --log_file offline_aug/100_subset_train_x1/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_aug/100_subset_train_x10/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_aug/100_subset_train_x10/training_spec.txt -k tlt_encode --log_file offline_aug/100_subset_train_x10/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_aug/100_subset_train_x20/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_aug/100_subset_train_x20/training_spec.txt -k tlt_encode --log_file offline_aug/100_subset_train_x20/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc90c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_aug/500_subset_train_x1/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_aug/500_subset_train_x1/training_spec.txt -k tlt_encode --log_file offline_aug/500_subset_train_x1/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_aug/500_subset_train_x10/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_aug/500_subset_train_x10/training_spec.txt -k tlt_encode --log_file offline_aug/500_subset_train_x10/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!faster_rcnn train -e offline_aug/500_subset_train_x20/training_spec.txt -k tlt_encode\n",
    "!faster_rcnn evaluate -e offline_aug/500_subset_train_x20/training_spec.txt -k tlt_encode --log_file offline_aug/500_subset_train_x20/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc8c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
