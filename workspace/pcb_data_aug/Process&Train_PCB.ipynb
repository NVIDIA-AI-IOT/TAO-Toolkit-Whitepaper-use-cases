{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e1cef3",
   "metadata": {},
   "source": [
    "# Data Augmenation On The PCB Defect Dataset\n",
    "\n",
    "This notebook contains all the code needed to use TAO augmenation on subsets of the PCB defect dataset to showcase how augmenatation can be used to improve KPIs for small datasets. \n",
    "\n",
    "This notebook requires the TAO Launcher, Docker and NGC to be setup\n",
    "\n",
    "The github readme has steps on setting up the prerequisites \n",
    "\n",
    "This notebook also requires preprocess_pcb.py to be in the same directory to function. \n",
    "\n",
    "This notebook takes the following steps\n",
    "1) Download and unpack the PCB defect dataset\n",
    "\n",
    "2) Convert the dataset to kitti format \n",
    "\n",
    "3) Split the dataset into test and train subsets\n",
    "\n",
    "4) Map local directories the the TAO launcher\n",
    "\n",
    "5) Generate offline augmenation spec file and apply augmentation to the training sets\n",
    "\n",
    "6) Generate TF Records for the test and training sets\n",
    "\n",
    "7) Downloads pretrained object detection weights needed for the trainings\n",
    "\n",
    "8) Launch trainings and evaluation\n",
    "\n",
    "The last section of this notebook contains all the commands needed to run training and evaluation on all 6 datasets.  \n",
    "Steps 1-7 only need to run 1 time. The trainings in step 7 can be run in any order once steps 1-6 have successfully run. \n",
    "A common test set of 500 images is used for validation on all trainings\n",
    "\n",
    "Datasets\n",
    "100 subset x1  \n",
    "100 subset x10  \n",
    "100 subset x20  \n",
    "500 subset x1  \n",
    "500 subset x10  \n",
    "500 subset x20  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f31942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74288a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from preprocess_pcb import convert_annotation, create_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths relative to local repository\n",
    "repo_home = os.path.join(os.getcwd(), \"../../\")\n",
    "model_home = os.path.join(repo_home, \"workspace/models\")\n",
    "dataset_home = os.path.join(repo_home, \"datasets/pcb_defect\")\n",
    "exp_home = os.path.join(repo_home, \"workspace/pcb_data_aug\")\n",
    "\n",
    "#paths for inside the container\n",
    "dataset_home_cont = \"/datasets/pcb_defect/\"\n",
    "exp_home_cont = \"/tlt_exp/pcb_data_aug/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514856e",
   "metadata": {},
   "source": [
    "## Download and unpack the PCB defect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $dataset_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download and unzip\n",
    "!wget https://www.dropbox.com/s/h0f39nyotddibsb/VOC_PCB.zip \n",
    "!unzip VOC_PCB.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd6e64",
   "metadata": {},
   "source": [
    "## Convert the dataset to kitti format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup folders for dataset images and labels\n",
    "os.makedirs(\"original/images\", exist_ok=True)\n",
    "os.makedirs(\"original/labels\", exist_ok=True)\n",
    "!cp -r VOC_PCB/JPEGImages/. original/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2132174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Paths and make label folder\n",
    "xml_label_path = \"VOC_PCB/Annotations\"\n",
    "kitti_label_output = \"original/labels\"\n",
    "\n",
    "#Convert labels to kitti and put into output folder\n",
    "for x in os.listdir(xml_label_path):\n",
    "    current_label_path = os.path.join(xml_label_path, x)\n",
    "    convert_annotation(current_label_path, kitti_label_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99a767",
   "metadata": {},
   "source": [
    "## Split the dataset into test and train subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb578f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup folders for dataset subset\n",
    "test_500 = os.path.join(exp_home, \"test_500_list.txt\")\n",
    "train_100 = os.path.join(exp_home, \"train_100_list.txt\")\n",
    "train_500 = os.path.join(exp_home, \"train_500_list.txt\")\n",
    "\n",
    "\n",
    "os.makedirs(\"500_subset_test_x1\", exist_ok=True)\n",
    "os.makedirs(\"100_subset_train_x1\", exist_ok=True)\n",
    "os.makedirs(\"500_subset_train_x1\", exist_ok=True)\n",
    "\n",
    "#Create the subsets based on predefined lists\n",
    "create_subset(\"original\", test_500, \"500_subset_test_x1\")\n",
    "create_subset(\"original\", train_100, \"100_subset_train_x1\")\n",
    "create_subset(\"original\", train_500, \"500_subset_train_x1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c23698",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map local directories the the TAO launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b760e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.path.join(repo_home, \"datasets\"),\n",
    "            \"destination\": \"/datasets\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.path.join(repo_home, \"workspace\"),\n",
    "            \"destination\": \"/tlt_exp\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd4fc5",
   "metadata": {},
   "source": [
    "## Generate offline augmenation spec file and apply augmentation to the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_pcb import gen_random_aug_spec, combine_kitti, visualize_images\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ea3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input dataset folder to augment, augment output folder and number of augmentations. Requires local paths and container paths\n",
    "#For each augment a randomized spec file and augmented dataset is produced\n",
    "#Also outputs a dataset with all combined augmentations\n",
    "def generate_augments(dataset_folder, dataset_folder_cont,  output_folder, output_folder_cont, num_augments):\n",
    "    for i in range(0,num_augments):\n",
    "        spec_out = os.path.join(output_folder, \"aug_spec\" + str(i) + \".txt\")\n",
    "        spec_out_cont = os.path.join(output_folder_cont, \"aug_spec\" + str(i) + \".txt\")\n",
    "        gen_random_aug_spec(600,600,\"jpg\", spec_out)\n",
    "        !cat $spec_out\n",
    "\n",
    "        aug_folder = os.path.join(output_folder, \"aug\" + str(i))\n",
    "        aug_folder_cont = os.path.join(output_folder_cont, \"aug\" + str(i))\n",
    "        !tao augment -a $spec_out_cont -o $aug_folder_cont -d $dataset_folder_cont\n",
    "\n",
    "        if i == 0:\n",
    "            d1 = dataset_folder\n",
    "            d2 = aug_folder\n",
    "            d3 = os.path.join(output_folder, \"combined_x2\")\n",
    "            combine_kitti(d1,d2,d3)\n",
    "        else:\n",
    "            d1 = os.path.join(output_folder, \"combined_x\" + str(i+1))\n",
    "            d2 = aug_folder\n",
    "            d3 = os.path.join(output_folder, \"combined_x\" + str(i+2))\n",
    "            combine_kitti(d1,d2,d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate augmentations for 100 image subset\n",
    "dataset_folder = \"100_subset_train_x1\" #folder for the existing dataset to be augmented. This folder will not be modified\n",
    "dataset_folder_cont = os.path.join(dataset_home_cont, \"100_subset_train_x1\")\n",
    "\n",
    "output_folder = \"100_subset_train_aug\" #folder for the augmented output. Does not need to exist\n",
    "output_folder_cont = os.path.join(dataset_home_cont, output_folder)\n",
    "\n",
    "num_augments = 19 #number of augmented datasets to generate\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "generate_augments(dataset_folder,dataset_folder_cont,output_folder, output_folder_cont, num_augments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7149675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display some of the augmented images\n",
    "#Rerun to see new images each time\n",
    "aug_choice = str(randint(0,num_augments-1))\n",
    "visualize_images(os.path.join(output_folder, \"aug\"+aug_choice+\"/images\"), num_images=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e720bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate augmentations for 500 image subset\n",
    "dataset_folder = \"500_subset_train_x1\" #folder for the existing dataset to be augmented. This folder will not be modified\n",
    "dataset_folder_cont = os.path.join(dataset_home_cont, \"500_subset_train_x1\")\n",
    "\n",
    "output_folder = \"500_subset_train_aug\" #folder for the augmented output. Does not need to exist\n",
    "output_folder_cont = os.path.join(dataset_home_cont, \"500_subset_train_aug\")\n",
    "\n",
    "num_augments = 19 #number of augmented datasets to generate\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "generate_augments(dataset_folder, dataset_folder_cont, output_folder, output_folder_cont, num_augments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9402e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display some of the augmented images\n",
    "#Rerun to see new images each time\n",
    "aug_choice = str(randint(0,num_augments-1))\n",
    "visualize_images(os.path.join(output_folder, \"aug\"+aug_choice+\"/images\"), num_images=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place important datasets in the dataset folder\n",
    "\n",
    "!mv 100_subset_train_aug/combined_x10 100_subset_train_x10\n",
    "!mv 100_subset_train_aug/combined_x20 100_subset_train_x20\n",
    "\n",
    "!mv 500_subset_train_aug/combined_x10 500_subset_train_x10\n",
    "!mv 500_subset_train_aug/combined_x20 500_subset_train_x20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e8aaa",
   "metadata": {},
   "source": [
    "## Generate TF Records for the test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0905f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the tf record config as a string with the given dataset path\n",
    "#root directory path must be inside the container\n",
    "def gen_tf_spec(dataset_path):\n",
    "\n",
    "    spec_str = f\"\"\"\n",
    "    kitti_config {{\n",
    "      root_directory_path: \"/datasets/pcb_defect/{dataset_path}\"\n",
    "      image_dir_name: \"images\"\n",
    "      label_dir_name: \"labels\"\n",
    "      image_extension: \".jpg\"\n",
    "      partition_mode: \"random\"\n",
    "      num_partitions: 2\n",
    "      val_split: 20\n",
    "      num_shards: 10\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return spec_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all datasets to generate tf records\n",
    "dataset_paths = [\"500_subset_test_x1\", \"500_subset_train_x1\", \"500_subset_train_x10\", \"500_subset_train_x20\", \"100_subset_train_x1\", \"100_subset_train_x10\", \"100_subset_train_x20\"]\n",
    "for path in dataset_paths:\n",
    "    record_path = os.path.join(dataset_home, path, \"tfrecord_spec.txt\")\n",
    "    record_path_cont = os.path.join(dataset_home_cont, path, \"tfrecord_spec.txt\")\n",
    "    \n",
    "    record_output = os.path.join(dataset_home, path, \"tfrecords_rcnn/\")\n",
    "    record_output_cont = os.path.join(dataset_home_cont, path, \"tfrecords_rcnn/\")\n",
    "    \n",
    "    print(\"************\" + record_path)\n",
    "    with open(record_path, \"w+\") as spec:\n",
    "        spec.write(gen_tf_spec(path))\n",
    "    !tao faster_rcnn dataset_convert -d $record_path_cont -o $record_output_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa807a1f",
   "metadata": {},
   "source": [
    "## Downloads pretrained object detection weights needed for the trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84508546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires NGC to be configured\n",
    "os.makedirs(os.path.join(model_home, \"fasterRCNN\"), exist_ok=True)\n",
    "%cd $model_home/fasterRCNN\n",
    "!ngc registry model download-version \"nvidia/tlt_pretrained_object_detection:resnet18\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7487f9",
   "metadata": {},
   "source": [
    "## Launch trainings and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030573a",
   "metadata": {},
   "source": [
    "Each cell in this section will train and evaluate on 1 dataset in the experiment. The results will be output to the respective experiment folder. \n",
    "\n",
    "The trainings may take several hours depending on your hardware. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_cont = os.path.join(exp_home_cont, \"experiments\")\n",
    "experiments = os.path.join(exp_home, \"experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d755c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_aug/100_subset_train_x1/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_aug/100_subset_train_x1/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_aug/100_subset_train_x1/eval_log.txt\n",
    "!cat $experiments/offline_aug/100_subset_train_x1/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_aug/100_subset_train_x10/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_aug/100_subset_train_x10/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_aug/100_subset_train_x10/eval_log.txt\n",
    "!cat $experiments/offline_aug/100_subset_train_x10/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_aug/100_subset_train_x20/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_aug/100_subset_train_x20/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_aug/100_subset_train_x20/eval_log.txt\n",
    "!cat $experiments/offline_aug/100_subset_train_x20/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc90c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_aug/500_subset_train_x1/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_aug/500_subset_train_x1/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_aug/500_subset_train_x1/eval_log.txt\n",
    "!cat $experiments/offline_aug/500_subset_train_x1/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_aug/500_subset_train_x10/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_aug/500_subset_train_x10/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_aug/500_subset_train_x10/eval_log.txt\n",
    "!cat $experiments/offline_aug/500_subset_train_x10/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_aug/500_subset_train_x20/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_aug/500_subset_train_x20/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_aug/500_subset_train_x20/eval_log.txt\n",
    "!cat $experiments/offline_aug/500_subset_train_x20/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778056e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_online_aug/100_subset_train_x1/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_online_aug/100_subset_train_x1/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_online_aug/100_subset_train_x1/eval_log.txt\n",
    "!cat $experiments/offline_online_aug/100_subset_train_x1/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_online_aug/100_subset_train_x10/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_online_aug/100_subset_train_x10/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_online_aug/100_subset_train_x10/eval_log.txt\n",
    "!cat $experiments/offline_online_aug/100_subset_train_x10/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_online_aug/100_subset_train_x20/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_online_aug/100_subset_train_x20/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_online_aug/100_subset_train_x20/eval_log.txt\n",
    "!cat $experiments/offline_online_aug/100_subset_train_x20/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2257d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_online_aug/500_subset_train_x1/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_online_aug/500_subset_train_x1/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_online_aug/500_subset_train_x1/eval_log.txt\n",
    "!cat $experiments/offline_online_aug/500_subset_train_x1/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef25d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_online_aug/500_subset_train_x10/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_online_aug/500_subset_train_x10/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_online_aug/500_subset_train_x10/eval_log.txt\n",
    "!cat $experiments/offline_online_aug/500_subset_train_x10/eval_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao faster_rcnn train -e $experiments_cont/offline_online_aug/500_subset_train_x20/training_spec.txt -k tlt_encode\n",
    "!tao faster_rcnn evaluate -e $experiments_cont/offline_online_aug/500_subset_train_x20/training_spec.txt -k tlt_encode --log_file $experiments_cont/offline_online_aug/500_subset_train_x20/eval_log.txt\n",
    "!cat $experiments/offline_online_aug/500_subset_train_x20/eval_log.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
